{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Project\n",
    "\n",
    "As a Data Scientist, you are tasked to help these users find the most interesting articles\n",
    "according to their preferred topics. You have a ***training dataset containing about 9500 news\n",
    "articles, each assigned to one of the above topics***. In addition, (as in real life situation) the\n",
    "dataset contains about ***48% of irrelevant articles*** (marked as IRRELEVANT) that do not\n",
    "belong to any of the topics; hence the users are not interested in them. The distribution of\n",
    "articles over topics is not uniform. There are some topics with large number of articles, and\n",
    "some with very small number.\n",
    "\n",
    "One day, 500 new articles have been published. This is your test set that has similar article\n",
    "distribution over topics to the training set. ***Your task is to suggest up to 10 of the most relevant\n",
    "articles from this set of 500 to each user***. The number of suggestions is limited to 10, because,\n",
    "presumably, the users do not want to read more suggestions. It is possible, however, that some\n",
    "topics within this test set have less than 10 articles. You also do not want to suggest 10 articles\n",
    "if they are unlikely to be relevant, because you are concerned that the users may get\n",
    "discouraged and stop using your application altogether. Therefore you need to take a balanced\n",
    "approach, paying attention to not suggesting too many articles for rare topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "import collections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import text data\n",
    "raw_training = pd.read_csv(\"training.csv\")\n",
    "raw_testing = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Create bag of words\n",
    "count = CountVectorizer()\n",
    "bag_of_words = count.fit_transform(raw_training[\"article_words\"])\n",
    "\n",
    "# Create feature matrix\n",
    "X = bag_of_words\n",
    "\n",
    "# Create bag of words\n",
    "y = raw_training[\"topic\"]\n",
    "\n",
    "#######################Resampling Dataset#######################\n",
    "\n",
    "\n",
    "# Reducing the effect imbalnced by deleting some irrelevant class\n",
    "# \"Irrelevant\" classe has 4734 samples in the training data, try to reduce it into 2000\n",
    "irrelevant = raw_training[raw_training[\"topic\"] == \"IRRELEVANT\"]\n",
    "remove_n = 2734\n",
    "drop_indices = np.random.choice(irrelevant.index, remove_n, replace=False)\n",
    "irrelevant = irrelevant.drop(drop_indices)\n",
    "zx\n",
    "reduce_training =  pd.concat([raw_training[raw_training[\"topic\"] != \"IRRELEVANT\"], irrelevant],ignore_index=True)\n",
    "reduce_bag_of_words = count.fit_transform(reduce_training[\"article_words\"])\n",
    "R_X = reduce_bag_of_words\n",
    "R_y = reduce_training[\"topic\"]\n",
    "\n",
    "# Icreasing the minor classes\n",
    "# Increasing \n",
    "topic_class = raw_training[raw_training[\"topic\"] != \"IRRELEVANT\"]\n",
    "increase_training = pd.concat([topic_class, topic_class, topic_class, raw_training[raw_training[\"topic\"] == \"IRRELEVANT\"]], ignore_index=True)\n",
    "increase_bag_of_words = count.fit_transform(increase_training[\"article_words\"])\n",
    "I_X = increase_bag_of_words\n",
    "I_y = increase_training[\"topic\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       FOREX MARKETS\n",
       "1       MONEY MARKETS\n",
       "2              SPORTS\n",
       "3       FOREX MARKETS\n",
       "4          IRRELEVANT\n",
       "            ...      \n",
       "9495          DEFENCE\n",
       "9496       IRRELEVANT\n",
       "9497    FOREX MARKETS\n",
       "9498       IRRELEVANT\n",
       "9499    FOREX MARKETS\n",
       "Name: topic, Length: 9500, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Features:' , count.get_feature_names())# 檢視feature names\n",
    "#print('Values: \\n', X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Method\n",
    "Defining different model function to be used later in the k fold validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using specific model \"method\", return specifc score \"score\" by cross validation\n",
    "\n",
    "def Model_Score (X, y, method, k):\n",
    "    clf = method\n",
    "    \n",
    "    accuracy_scores = cross_val_score(clf, X, y, cv=k, scoring=\"accuracy\")\n",
    "    precision_scores = cross_val_score(clf, X, y, cv=k, scoring=\"precision_macro\")\n",
    "    recall_scores = cross_val_score(clf, X, y, cv=k, scoring=\"recall_macro\")\n",
    "    f1_scores = cross_val_score(clf, X, y, cv=k, scoring=\"f1_macro\")\n",
    "    \n",
    "    return np.mean(accuracy_scores), np.mean(precision_scores), np.mean(recall_scores), np.mean(f1_scores)\n",
    "\n",
    "#def Model_report ()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model (Naive Bayes)\n",
    "Using K-fold validation to split the training data and validation data. Use the average score of the validation sets to evaluate the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# creating 10 fold for k-fold validation\n",
    "#A = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [5, 6], [7, 2], [5, 4], [7, 5], [10, 4]]) \n",
    "b = np.array([1, 2, 3, 4,5,6,7,8,9,10])\n",
    "\n",
    "# k-fold split number 10 \n",
    "k = 10\n",
    "\n",
    "# without doing any data cleaning\n",
    "\n",
    "bernoulliNB_accuracy, bernoulliNB_precision, bernoulliNB_recall, bernoulliNB_f1 = Model_Score(X, y, BernoulliNB(), 10)\n",
    "multinomialNB_accuracy, multinomialNB_precision, multinomialNB_recall, multinomialNB_f1 = Model_Score(X, y, MultinomialNB(), 10)\n",
    "\n",
    "\n",
    "# multinomialNB using uniformed distribution\n",
    "multiNB_accuracy2, multiNB_precision2, multiNB_recall2, multiNB_f12 = Model_Score(X, y, MultinomialNB(fit_prior = False), 10)\n",
    "\n",
    "\n",
    "# reduce irrelevant samples\n",
    "R_accuracy, R_precision, R_recall, R_f1 = Model_Score(R_X, R_y, MultinomialNB(), 10)\n",
    "\n",
    "\n",
    "# Icreasing the minor classes\n",
    "I_accuracy, I_precision, I_recall, I_f1 = Model_Score(I_X, I_y, MultinomialNB(), 10)\n",
    "\n",
    "\n",
    "# Try Classification report\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "clf_BernoulliNB = BernoulliNB()\n",
    "model_BernoulliNB = clf_BernoulliNB.fit(X_train, y_train)\n",
    "\n",
    "clf_MultinomialNB = MultinomialNB()\n",
    "model_MultinomialNB = clf_MultinomialNB.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "predicted_BernoulliNB = model_BernoulliNB.predict(X_valid)\n",
    "predicted_MultinomialNB = model_MultinomialNB.predict(X_valid)\n",
    "\n",
    "I_X_train, I_X_valid, I_y_train, I_y_valid = train_test_split(I_X, I_y, test_size=0.20, random_state=42)\n",
    "model_MultinomialNB_2 = clf_MultinomialNB.fit(I_X_train, I_y_train)\n",
    "predicted_MultinomialNB_2 = model_MultinomialNB_2.predict(I_X_valid)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# still need to deal with \n",
    "\n",
    "\n",
    "# 1. irrelevant articles\n",
    "\n",
    "# 2. The distribution of topics are not uniform\n",
    "\n",
    "# 3. select the features\n",
    "\n",
    "# 4. maybe can give penalty to the misclassifying\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# multinomialNB_accuracy = Model_Score(X, y, MultinomialNB(), 10, \"accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================\n",
      "Without doing any data cleaning, the score of bernoulliNB,\n",
      "accuracy:  0.7038942167520311\n",
      "precision: 0.37212369276862634\n",
      "recall:    0.2819154032170111\n",
      "f1:        0.28388746010825894\n",
      "====================================================================================\n",
      "Without doing any data cleaning, the score of multinomialNB,\n",
      "accuracy:  0.7358161832731782\n",
      "precision: 0.6306736067435463\n",
      "recall:    0.5526188603489018\n",
      "f1:        0.5583352913640214\n",
      "====================================================================================\n",
      "Setting uniformed prior, the score of multinomialNB,\n",
      "accuracy:  0.7338169401702015\n",
      "precision: 0.6325643464367477\n",
      "recall:    0.5709569773293275\n",
      "f1:        0.5712769594923721\n",
      "====================================================================================\n",
      "Reduce the case of irrelevant, the score of multinomialNB,\n",
      "accuracy:  0.7106463220517194\n",
      "precision: 0.7075125677990888\n",
      "recall:    0.6049673605345067\n",
      "f1:        0.6255152766267569\n",
      "====================================================================================\n",
      "Increase(Copy) the case of topic classes, the score of multinomialNB,\n",
      "accuracy:  0.8046992057147376\n",
      "precision: 0.8135680332176245\n",
      "recall:    0.8751822879751108\n",
      "f1:        0.8346417211219871\n",
      "\n",
      "Classification Report for multinomialNB after Resampling Dataset:\n",
      "\n",
      "                                  precision    recall  f1-score   support\n",
      "\n",
      "      ARTS CULTURE ENTERTAINMENT       0.84      0.89      0.87        73\n",
      "BIOGRAPHIES PERSONALITIES PEOPLE       0.82      0.86      0.84       123\n",
      "                         DEFENCE       0.69      0.96      0.80       158\n",
      "                DOMESTIC MARKETS       0.76      0.92      0.83        63\n",
      "                   FOREX MARKETS       0.53      0.67      0.59       496\n",
      "                          HEALTH       0.82      0.95      0.88       115\n",
      "                      IRRELEVANT       0.93      0.68      0.78       947\n",
      "                   MONEY MARKETS       0.73      0.73      0.73       988\n",
      "          SCIENCE AND TECHNOLOGY       1.00      0.83      0.91        41\n",
      "                  SHARE LISTINGS       0.78      0.94      0.85       133\n",
      "                          SPORTS       0.97      0.98      0.98       670\n",
      "\n",
      "                        accuracy                           0.79      3807\n",
      "                       macro avg       0.81      0.86      0.82      3807\n",
      "                    weighted avg       0.81      0.79      0.79      3807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"====================================================================================\")\n",
    "print(\"Without doing any data cleaning, the score of bernoulliNB,\\naccuracy:  \" + str(bernoulliNB_accuracy) +\n",
    "     \"\\nprecision: \" + str(bernoulliNB_precision) + \"\\nrecall:    \" + str(bernoulliNB_recall) + \"\\nf1:        \" +\n",
    "     str(bernoulliNB_f1))\n",
    "#print(\"\\nClassification Report for bernoulliNB:\\n\")\n",
    "#print(classification_report(y_valid, predicted_BernoulliNB))\n",
    "\n",
    "print(\"====================================================================================\")\n",
    "\n",
    "print(\"Without doing any data cleaning, the score of multinomialNB,\\naccuracy:  \" + str(multinomialNB_accuracy) +\n",
    "     \"\\nprecision: \" + str(multinomialNB_precision) + \"\\nrecall:    \" + str(multinomialNB_recall) + \"\\nf1:        \" +\n",
    "     str(multinomialNB_f1))\n",
    "#print(\"\\nClassification Report for multinomialNB:\\n\")\n",
    "#print(classification_report(y_valid, predicted_MultinomialNB))\n",
    "\n",
    "print(\"====================================================================================\")\n",
    "\n",
    "print(\"Setting uniformed prior, the score of multinomialNB,\\naccuracy:  \" + str(multiNB_accuracy2) +\n",
    "     \"\\nprecision: \" + str(multiNB_precision2) + \"\\nrecall:    \" + str(multiNB_recall2) + \"\\nf1:        \" +\n",
    "     str(multiNB_f12))\n",
    "#print(\"\\nClassification Report for multinomialNB:\\n\")\n",
    "#print(classification_report(y_valid, predicted_MultinomialNB))\n",
    "\n",
    "print(\"====================================================================================\")\n",
    "\n",
    "print(\"Reduce the case of irrelevant, the score of multinomialNB,\\naccuracy:  \" + str(R_accuracy) +\n",
    "     \"\\nprecision: \" + str(R_precision) + \"\\nrecall:    \" + str(R_recall) + \"\\nf1:        \" +\n",
    "     str(R_f1))\n",
    "\n",
    "print(\"====================================================================================\")\n",
    "\n",
    "print(\"Increase(Copy) the case of topic classes, the score of multinomialNB,\\naccuracy:  \" + str(I_accuracy) +\n",
    "     \"\\nprecision: \" + str(I_precision) + \"\\nrecall:    \" + str(I_recall) + \"\\nf1:        \" +\n",
    "     str(I_f1))\n",
    "\n",
    "print(\"\\nClassification Report for multinomialNB after Resampling Dataset:\\n\")\n",
    "print(classification_report(I_y_valid, predicted_MultinomialNB_2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Test Try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncv = CountVectorizer()\\nX_t = count.fit_transform(raw_testing[\"article_words\"])\\ny_t = raw_testing[\"topic\"]\\n\\nprint(type(X_valid))\\nprint(len(X_valid.toarray()[0]))\\nprint(type(X_t))\\nprint(len(X_t.toarray()[0]))\\n\\nproba_y = model_MultinomialNB_2.predict(X_t)\\n\\n<class \\'scipy.sparse.csr.csr_matrix\\'>\\n35822\\n<class \\'scipy.sparse.csr.csr_matrix\\'>\\n52\\n\\nValueError: dimension mismatch\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating a test for test data separately would result in dimension mismatch\n",
    "\n",
    "'''\n",
    "cv = CountVectorizer()\n",
    "X_t = count.fit_transform(raw_testing[\"article_words\"])\n",
    "y_t = raw_testing[\"topic\"]\n",
    "\n",
    "print(type(X_valid))\n",
    "print(len(X_valid.toarray()[0]))\n",
    "print(type(X_t))\n",
    "print(len(X_t.toarray()[0]))\n",
    "\n",
    "proba_y = model_MultinomialNB_2.predict(X_t)\n",
    "\n",
    "<class 'scipy.sparse.csr.csr_matrix'>\n",
    "35822\n",
    "<class 'scipy.sparse.csr.csr_matrix'>\n",
    "52\n",
    "\n",
    "ValueError: dimension mismatch\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19032\n",
      "19532\n",
      "19532\n"
     ]
    }
   ],
   "source": [
    "# append raw_testing[\"article_words\"] and raw_testing[\"topic\"] to get consist dimension in the test\n",
    "topic_class = raw_training[raw_training[\"topic\"] != \"IRRELEVANT\"]\n",
    "increase_training = pd.concat([topic_class, topic_class, topic_class, raw_training[raw_training[\"topic\"] == \"IRRELEVANT\"]], ignore_index=True)\n",
    "train_size = len(increase_training)\n",
    "print(train_size)\n",
    "result = increase_training[\"article_words\"].append(raw_testing[\"article_words\"])\n",
    "print(len(result))\n",
    "\n",
    "increase_bag_of_words = count.fit_transform(result)\n",
    "I_X = increase_bag_of_words\n",
    "I_y = increase_training[\"topic\"].append(raw_testing[\"topic\"])\n",
    "\n",
    "print(len(I_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARTS CULTURE ENTERTAINMENT\n",
      "BIOGRAPHIES PERSONALITIES PEOPLE\n",
      "DEFENCE\n",
      "DOMESTIC MARKETS\n",
      "FOREX MARKETS\n",
      "HEALTH\n",
      "IRRELEVANT\n",
      "MONEY MARKETS\n",
      "SCIENCE AND TECHNOLOGY\n",
      "SHARE LISTINGS\n",
      "SPORTS\n"
     ]
    }
   ],
   "source": [
    "for c in model_MultinomialNB_2.classes_:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.94627027e-034 1.48561093e-045 4.14253628e-033 ... 4.03153273e-039\n",
      "  9.99999998e-001 2.79687735e-063]\n",
      " [8.41709112e-004 3.85527320e-005 2.07838284e-008 ... 1.64175632e-014\n",
      "  1.57817415e-009 7.24703136e-015]\n",
      " [4.47199805e-132 1.73460607e-128 9.77617803e-134 ... 4.45934322e-168\n",
      "  4.36857252e-085 8.73359063e-159]\n",
      " ...\n",
      " [1.38604807e-052 6.25993590e-050 1.22171378e-059 ... 3.14548643e-062\n",
      "  4.56774797e-045 2.25874435e-064]\n",
      " [2.29436041e-026 2.99121657e-027 1.14242197e-025 ... 4.98703915e-035\n",
      "  1.00000000e+000 1.55102828e-027]\n",
      " [3.38511646e-100 3.21569833e-106 5.74821293e-116 ... 2.96622785e-128\n",
      "  1.06071350e-077 9.23401914e-089]]\n",
      "86 500 0.172\n"
     ]
    }
   ],
   "source": [
    "I_X_train = I_X[:train_size]\n",
    "I_X_test = I_X[train_size:]\n",
    "I_y_train = I_y[:train_size]\n",
    "I_y_test = I_y[train_size:]\n",
    "\n",
    "model_MultinomialNB_2 = clf_MultinomialNB.fit(I_X_train, I_y_train)\n",
    "proba_y = model_MultinomialNB_2.predict_proba(I_X_test)\n",
    "print(proba_y)\n",
    "count = 0\n",
    "recommendation = dict()\n",
    "\n",
    "for c in model_MultinomialNB_2.classes_:\n",
    "    recommendation[c] = dict()\n",
    "\n",
    "for i in range(len(proba_y)):\n",
    "    #print(proba_y[i])\n",
    "    #print(predicted_MultinomialNB_2[i])\n",
    "\n",
    "    proba = np.max(proba_y[i])\n",
    "    pred = predicted_MultinomialNB_2[i]\n",
    "    true = I_y_test.array[i]\n",
    "    id = 9501+i\n",
    "    #print(\"id \", id)\n",
    "    #print(\"pred \", pred)\n",
    "    #print(\"proba \", proba)\n",
    "    #print(\"true  \", true)\n",
    "    if(pred == true):\n",
    "        count+=1\n",
    "        recommendation[pred][id] = proba\n",
    "print(count, len(proba_y), count/len(proba_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARTS CULTURE ENTERTAINMENT\n",
      "BIOGRAPHIES PERSONALITIES PEOPLE\n",
      "DEFENCE\n",
      "9616 1.0\n",
      "DOMESTIC MARKETS\n",
      "FOREX MARKETS\n",
      "9525 0.9999999939433337\n",
      "9551 0.9999999905656978\n",
      "9659 0.9999999873775778\n",
      "9529 0.9927546477636874\n",
      "9708 0.9743567487101559\n",
      "9733 0.8236482033513559\n",
      "HEALTH\n",
      "IRRELEVANT\n",
      "9515 1.0\n",
      "9527 1.0\n",
      "9605 1.0\n",
      "9624 1.0\n",
      "9741 1.0\n",
      "9754 1.0\n",
      "9781 1.0\n",
      "9790 1.0\n",
      "9794 1.0\n",
      "9797 1.0\n",
      "MONEY MARKETS\n",
      "9516 1.0\n",
      "9553 1.0\n",
      "9572 1.0\n",
      "9586 1.0\n",
      "9599 0.9999999999984084\n",
      "9916 0.9999999999982379\n",
      "9820 0.9999999999783427\n",
      "9748 0.9999999595761445\n",
      "9638 0.9999999444833365\n",
      "9550 0.9999999408703406\n",
      "SCIENCE AND TECHNOLOGY\n",
      "SHARE LISTINGS\n",
      "SPORTS\n",
      "9536 1.0\n",
      "9597 1.0\n",
      "9630 1.0\n",
      "9657 1.0\n",
      "9752 1.0\n",
      "9800 1.0\n",
      "9801 1.0\n",
      "9848 1.0\n",
      "9919 1.0\n",
      "9942 1.0\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "for key in recommendation:\n",
    "    d = recommendation[key];\n",
    "    sorted_d = dict(sorted(d.items(), key=operator.itemgetter(1),reverse=True))\n",
    "    print(key)\n",
    "    count = 0\n",
    "    for k in sorted_d:\n",
    "        print(k, sorted_d[k])\n",
    "        count+=1\n",
    "        if count == 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model (Decision Tree)\n",
    "Using K-fold validation to split the training data and validation data. Use the average score of the validation sets to evaluate the performance of the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "decisionTree_accuracy, decisionTree_precision, decisionTree_recall, decisionTree_f1 = Model_Score(X, y, DecisionTreeClassifier(), 10)\n",
    "\n",
    "\n",
    "clf_decisionTree = DecisionTreeClassifier()\n",
    "model_decisionTree = clf_decisionTree.fit(X_train, y_train)\n",
    "\n",
    "predicted_decisionTree = model_decisionTree.predict(X_valid)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================\\n\n",
      "Without doing any data cleaning, the score of decisionTree,\n",
      "accuracy:  0.6996523800897622\n",
      "precision: 0.5381255307309976\n",
      "recall:    0.49843501674178625\n",
      "f1:        0.5162767074832546\n",
      "\n",
      "Classification Report for decisionTree:\n",
      "\n",
      "                                  precision    recall  f1-score   support\n",
      "\n",
      "      ARTS CULTURE ENTERTAINMENT       0.64      0.32      0.42        22\n",
      "BIOGRAPHIES PERSONALITIES PEOPLE       0.31      0.21      0.25        39\n",
      "                         DEFENCE       0.56      0.45      0.50        44\n",
      "                DOMESTIC MARKETS       0.60      0.56      0.58        27\n",
      "                   FOREX MARKETS       0.40      0.40      0.40       174\n",
      "                          HEALTH       0.47      0.35      0.40        49\n",
      "                      IRRELEVANT       0.78      0.82      0.80       909\n",
      "                   MONEY MARKETS       0.59      0.60      0.60       344\n",
      "          SCIENCE AND TECHNOLOGY       0.45      0.28      0.34        18\n",
      "                  SHARE LISTINGS       0.45      0.47      0.46        47\n",
      "                          SPORTS       0.88      0.89      0.89       227\n",
      "\n",
      "                        accuracy                           0.69      1900\n",
      "                       macro avg       0.56      0.49      0.51      1900\n",
      "                    weighted avg       0.69      0.69      0.69      1900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n====================================================================================\\\\n\")\n",
    "print(\"Without doing any data cleaning, the score of decisionTree,\\naccuracy:  \" + str(decisionTree_accuracy) +\n",
    "     \"\\nprecision: \" + str(decisionTree_precision) + \"\\nrecall:    \" + str(decisionTree_recall) + \"\\nf1:        \" +\n",
    "     str(decisionTree_f1))\n",
    "print(\"\\nClassification Report for decisionTree:\\n\")\n",
    "print(classification_report(y_valid, predicted_decisionTree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'FOREX MARKETS': 845,\n",
       "         'MONEY MARKETS': 1673,\n",
       "         'SPORTS': 1102,\n",
       "         'IRRELEVANT': 4734,\n",
       "         'SHARE LISTINGS': 218,\n",
       "         'BIOGRAPHIES PERSONALITIES PEOPLE': 167,\n",
       "         'DOMESTIC MARKETS': 133,\n",
       "         'DEFENCE': 258,\n",
       "         'SCIENCE AND TECHNOLOGY': 70,\n",
       "         'HEALTH': 183,\n",
       "         'ARTS CULTURE ENTERTAINMENT': 117})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collections.Counter(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
